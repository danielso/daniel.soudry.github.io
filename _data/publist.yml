- title: "DropCompute: simple and more robust distributed synchronous training via compute variance reduction"
  image: DropComputePaper.JPG
  description: 
  authors: N. Giladi*, S. Gottlieb*, M. Shkolnik, A. Karnieli, R. Banner, E. Hoffer, K. Y. Levy, D. Soudry  (*Indicates equal contribution)
  link:
    url: https://arxiv.org/abs/2306.10598
    display:  NeurIPS 2023 (2023)
  highlight: 1
  news2: 

- title: "Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations"
  image: Alias-Free.JPG
  description: Although CNNs are believed to be invariant to translations, recent works have
shown this is not the case, due to aliasing effects that stem from downsampling
layers. The existing architectural solutions to prevent aliasing are partial since
they do not solve these effects, that originate in non-linearities. We propose an extended anti-aliasing method that tackles both downsampling and non-linear layers,
thus creating truly alias-free, shift-invariant CNNs1
. We show that the presented
model is invariant to integer as well as fractional (i.e., sub-pixel) translations, thus
outperforming other shift-invariant methods in terms of robustness to adversarial
translations.
  authors: H. Michaeli, T. Michaeli, D. Soudry
  link:
    url: https://arxiv.org/abs/2303.08085
    display: CVPR 2023 (2023)
  highlight: 1
  news1:  See more details about the paper <a href="https://hmichaeli.github.io/alias_free_convnets/"> Here </a>

- title: "The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks"
  image: TheImplicitPaper.JPG
  description: We study the type of solutions to which stochastic gradient descent converges when
used to train a single hidden-layer multivariate ReLU network with the quadratic
loss. Our results are based on a dynamical stability analysis. In the univariate
case, it was shown that linearly stable minima correspond to network functions
(predictors), whose second derivative has a bounded weighted L
1 norm. Notably,
the bound gets smaller as the step size increases, implying that training with a
large step size leads to ‘smoother’ predictors. Here we generalize this result to
the multivariate case, showing that a similar result applies to the Laplacian of the
predictor. We demonstrate the tightness of our bound on the MNIST dataset, and
show that it accurately captures the behavior of the solutions as a function of the
step size. Additionally, we prove a depth separation result on the approximation
power of ReLU networks corresponding to stable minima of the loss. Specifically,
although shallow ReLU networks are universal approximators, we prove that
stable shallow networks are not. Namely, there is a function that cannot be wellapproximated by stable single hidden-layer ReLU networks trained with a nonvanishing step size. This is while the same function can be realized as a stable
two hidden-layer ReLU network. Finally, we prove that if a function is sufficiently
smooth (in a Sobolev sense) then it can be approximated arbitrarily well using
shallow ReLU networks that correspond to stable solutions of gradient descent.
  authors: M. Shpigel Nacson*, R. Mulayoff*, G. Ongie, T. Michaeli, D. Soudry (*Indicates equal contribution)
  link:
    url: https://arxiv.org/abs/2306.17499
    display: ICLR 2023 (2023)
  highlight: 1

- title: "Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats"
  image: AccurateNeuralPaper.JPG
  description: Quantization of the weights and activations is one of the main methods to reduce
the computational footprint of Deep Neural Networks (DNNs) training. Current
methods enable 4-bit quantization of the forward phase. However, this constitutes
only a third of the training process. Reducing the computational footprint of the
entire training process requires the quantization of the neural gradients, i.e., the
loss gradients with respect to the outputs of intermediate neural layers.
Previous works separately showed that accurate 4-bit quantization of the neural gradients needs to (1) be unbiased and (2) have a log scale. However, no previous work
aimed to combine both ideas, as we do in this work. Specifically, we examine the
importance of having unbiased quantization in quantized neural network training,
where to maintain it, and how to combine it with logarithmic quantization. Based
on this, we suggest a logarithmic unbiased quantization (LUQ) method to quantize
both the forward and backward phases to 4-bit, achieving state-of-the-art results
in 4-bit training without the overhead. For example, in ResNet50 on ImageNet,
we achieved a degradation of 1.1%. We further improve this to a degradation
of only 0.32% after three epochs of high precision fine-tuning, combined with a
variance reduction method—where both these methods add overhead comparable
to previously suggested methods.
  authors: B. Chmiel, R. Banner, E. Hoffer, H. Ben Yaacov, D. Soudry
  link:
    url: https://openreview.net/forum?id=yTbNYYcopd
    display: ICLR 2023 (2023)
  highlight: 1
  news1: 
  news2: 

- title: "How catastrophic can catastrophic forgetting be in linear regression?"
  image: HowCatastrophicPaper.JPG
  description: To better understand catastrophic forgetting, we study fitting an overparameterized linear model
to a sequence of tasks with different input distributions. We analyze how much the model forgets
the true labels of earlier tasks after training on subsequent tasks, obtaining exact expressions and
bounds. We establish connections between continual learning in the linear setting and two other
research areas – alternating projections and the Kaczmarz method.
  authors:  I. Evron, E. Moroshko, R. Ward, N. Srebro, D. Soudry
  link:
    url: https://arxiv.org/abs/2205.09588
    display:  COLT 2022 (2022)
  highlight: 1
  news2:

- title: "A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks"
  image: StatisicalFrameworkPaper.JPG
  description: We frame Out Of Distribution (OOD) detection in DNNs as a
statistical hypothesis testing problem. Tests generated within our proposed framework combine evidence from the entire network. Unlike previous OOD detection
heuristics, this framework returns a p-value for each test sample. It is guaranteed
to maintain the Type I Error (T1E - incorrectly predicting OOD for an actual
in-distribution sample) for test data
  authors: M. Haroush, T. Frostig, R. Heller, D. Soudry
  link:
    url: https://openreview.net/forum?id=Oy9WeuZD51
    display:   ICLR 2022 (2022)
  highlight: 1
  news2: 

- title: "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks"
  image: AcceleratedSparse.JPG
  description: In order to allow for similar accelerations in the training phase, we suggest
a novel transposable fine-grained sparsity mask, where the same mask can be
used for both forward and backward passes
  authors: I. Hubara, B. Chmiel, M. Island, R. Banner, S. Naor, D. Soudry
  link:
    url: https://arxiv.org/abs/2102.08124
    display:  NeurIPS 2021 (2021)
  highlight: 1

- title: "Neural gradients are near-lognormal: understanding sparse and quantized training"
  image: NeuralGradientPaper.JPG
  description:s, we suggest two closed-form analytical
methods to reduce the computational and memory burdens of neural gradients. The
first method optimizes the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning
  authors: B. Chmiel*, L. Ben-Uri*, M. Shkolnik, E. Hoffer, R. Banner, D. Soudry  (*Indicates equal contribution) 
  link:
    url: https://openreview.net/forum?id=EoFNy62JGd
    display:  ICLR 2021 (2021)
  highlight: 1
  news2:


- title: "Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy"
  image: TBG_ARPES.png
  description: We provide a detailed asymptotic study of gradient flow trajectories and their
implicit optimization bias when minimizing the exponential loss over “diagonal
linear networks”
  authors:  E. Moroshko, S. Gunasekar, B. Woodworth, J. D. Lee, N. Srebro, D. Soudry
  link:
    url: https://arxiv.org/abs/2007.06738
    display:  NeurIPS 2020, Spotlight (3% acceptance rate) (2020)
  highlight: 1
  news2: 

- title: "The Knowledge Within: Methods for Data-Free Model Compression"
  image: TheKnowledgeWithin.JPG
  authors:  M. Haroush, I. Hubara, E. Hoffer, D. Soudry
  description: We present three methods for generating synthetic samples from trained models. Then, we demonstrate how these samples can be used to calibrate and fine-tune quantized models without using any real data in the process. 
  Our best performing method has a negligible accuracy degradation compared to the original training set. This method, which leverages intrinsic batch normalization layers' statistics of the trained model, can be used to evaluate data similarity. Our approach opens a path towards genuine data-free model compression, alleviating the need for training data during model deployment.
  link:
    url: https://arxiv.org/abs/1912.01274
    display:   CVPR 2020 (2020)
  highlight: 1
  news2:

- title: "At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?"
  image: AtStabilityEdgePaper.JPG
  authors: N. Giladi*, M. Shpigel Nacson*, E. Hoffer, D. Soudry  (*Indicates equal contribution)
  description: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods. Contributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic gradient descent algorithm. We derive closed-form rules on how the learning rate could be changed, while keeping the accessible set the same. Specifically, for high delay values, we find that the learning rate should be kept inversely proportional to the delay. We then extend this analysis to include momentum. We find momentum should be either turned off, or modified to improve training stability. 
  We provide empirical experiments to validate our theoretical findings.
  link:
    url: https://arxiv.org/abs/1909.12340
    display:  ICLR 2020 (2020)
  highlight: 1
  news2:

- title: "A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off"
  image: MeanFieldPaper.JPG
  description: We apply mean-field techniques to networks with quantized activations in order to evaluate the degree to which quantization degrades signal propagation at initialization. We derive initialization schemes which maximize signal propagation in such networks and suggest why this is helpful for generalization
  authors: Y. Blumenfeld, D. Gilboa, D. Soudry
  link:
    url: https://arxiv.org/abs/1906.00771
    display:  NeurIPS 2019 (2019)
  highlight: 1
  news2:

- title: "Post-training 4-bit quantization of convolution networks for rapid-deployment"
  image: PostTrainingPaper.JPG
  description: We target the quantization of both activations and
weights and suggest three complementary methods for minimizing quantization
error at the tensor level, two of whom obtain a closed-form analytical solution.
Combining these methods, our approach achieves accuracy that is just a few
percents less the state-of-the-art baseline across a wide range of convolutional
models
  authors: R. Banner, Y. Nahshan, D. Soudry
  link:
    url: https://arxiv.org/abs/1810.05723
    display:   NeurIPS 2019 (2019)
  highlight: 1
  news2: The source code to replicate all experiments is available on <a href = "https://github.com/submission2019/cnn-quantization"> GitHub </a> 

- title: "How do infinite width bounded norm networks look in function space?"
  image: HowDoInfinite.JPG
  description:  We consider the question of what functions can be captured by ReLU networks with an
unbounded number of units (infinite width), but where the overall network Euclidean norm
(sum of squares of all weights in the system, except for an unregularized bias term for each
unit) is bounded; or equivalently what is the minimal norm required to approximate a given
function.
  authors: P. Savarese, I. Evron, D. Soudry, N. Srebro
  link:
    url: https://arxiv.org/abs/1902.05040
    display: COLT 2019 (2019)
  highlight: 1
  news2: 

  
- title: "Scalable Methods for 8-bit Training of Neural Networks"
  image: ScalableMethodsPaper.JPG
  description: Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied.
  To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset.
  authors: R. Banner, I. Hubara, E. Hoffer, D. Soudry
  link:
    url: https://arxiv.org/abs/1805.11046
    display:  NeurIPS 2018 (2018)
  highlight: 1
  news2:

- title: "The Implicit Bias of Gradient Descent on Separable Data"
  image: TheImplicitBiasofGradientDescent.JPG
  description: We show that gradient descent on an unregularized logistic regression problem, for
almost all separable datasets, converges to the same direction as the max-margin
solution. The result generalizes also to other monotone decreasing loss functions
with an infimum at infinity, and we also discuss a multi-class generalizations to the
cross entropy loss. Furthermore, we show this convergence is very slow, and only
logarithmic in the convergence of the loss itself. This can help explain the benefit
of continuing to optimize the logistic or cross-entropy loss even after the training
error is zero and the training loss is extremely small, and, as we show, even if the
validation loss increases. Our methodology can also aid in understanding implicit
regularization in more complex models and with other optimization methods.

  authors: D. Soudry, E. Hoffer, M. Shpigel Nacson, N. Srebro
  link:
    url: https://openreview.net/pdf?id=r1q7n9gAb
    display: ICLR 2018 (2018)
  highlight: 1
  news2:

- title: "Binarized Neural Networks"
  image: BinarizedNeuralNetworksPaper.JPG
  description: 
  authors: I. Hubara*, M. Courbariaux*, D. Soudry, R. El-Yaniv, Y. Bengio (*Indicates equal contribution)
  link:
    url: https://arxiv.org/abs/1602.02830
    display:  NIPS 2016 (2016)
  highlight: 1
  news1: <a href="https://aip.scitation.org/doi/10.1063/1.5043267">Instrument details in Rev. Sci. Instrum. 89, 093709 (2018)</a>
  news2: 

  
- title: "Amplifier for scanning tunneling microscopy at MHz frequencies"
  image: Fano.png
  description: We develop, build and test a novel amplifier circuit capable of measuring the tunneling current in the MHz regime while simultaneously performing conventional STM measurements. We are looking forward to performing scanning noise spectroscopy on quantum materials!
  authors: KM Bastiaans, T Benschop, D Chatzopoulos, D Cho, Q Dong, Y Jin, MP Allan
  link:
    url: https://aip.scitation.org/doi/10.1063/1.5043267
    display:  Rev. Sci. Instrum. 89, 093709 (2018)
  highlight: 0
  news2:

- title: "Revisiting quasiparticle scattering interference in high-temperature superconductors: the problem of narrow peaks"
  image: dummy.png
  description: ""
  authors: MA Sulangi, MP Allan, J Zaanen
  link:
    url: https://journals.aps.org/prb/abstract/10.1103/PhysRevB.96.134507
    display:  PRB 96, 134507 (2017)
  highlight: 0
  news2: Coverd in Superconductor Week, November 30, 2017 Vol. 31, No. 10

- title: Robust procedure for creating and characterizing the atomic structure of scanning tunneling microscope tips
  image: dummy.png
  description: ""
  authors: S Tewari, KM Bastiaans, MP Allan, JM van Ruitenbeek
  link:
    url: http://www.beilstein-journals.org/bjnano/content/pdf/2190-4286-8-238.pdf
    display:  Beilstein J. Nanotechnol. 8, 2389 (2017)
  highlight: 0
  news2:

- title: Creating better superconductors by periodic nanopatterning
  image: SCAUweb.jpg
  description:
    We  propose  an approach to transform a ‘pristine’ material into a better (meta-) superconductor by making use of modern fabrication techniques -- designing and engineering the electronic properties of thin films via periodic patterning on the nanoscale.
  authors:
    MP Allan, MH Fischer, O Ostojic, A Andringa
  link:
    url: https://scipost.org/SciPostPhys.3.2.010/pdf
    display: SciPost Phys. 3, 010 (2017)
  highlight: 1

- title: Poor electronic screening in lightly doped Mott insulators observed with scanning tunneling microscopy
  image: TIBB.png
  description:
    We develop a simple model to interpret STM results on lightly doped Mott insulators, and find that the effective Mott gap measured by STM correspons to the one measured with other techniques when taking into account the poor electronic screeing.
  authors:
    I Battisti, V Fedoseev, KM Bastiaans, A de la Torre, RS Perry, F Baumberger, MP Allan
  link:
    url: https://journals.aps.org/prb/abstract/10.1103/PhysRevB.95.235141
    display: Phys. Rev. B 95, 235141, Editors' suggestion (2017)
  highlight: 0

- title: Universality of pseudo gap and emergent order in lightly doped Mott insulators
  image: Mott_phase_sep.jpg
  description:
    We discover of electronic order, pseudo gap phase separation and an impurity-band Mott transition in an iridate compound, cuprate-style. Along the way, we gained unique knowledge on how a Mott state collapses.
  authors:
    I. Battisti* & K.M. Bastiaans*, V. Fedoseev, A. de la Torre, N. Iliopoulos, A. Tamai, E.C. Hunter, R.S. Perry, J. Zaanen, F. Baumberger, M.P. Allan
  link:
    url: http://www.nature.com/nphys/journal/vaop/ncurrent/full/nphys3894.html
    display: Nature Physics 13, 21 (2017)
  highlight: 1
  news2: See also <a href="https://www.universiteitleiden.nl/en/news/2016/09/melting-of-frozen-electrons-visualized"> Leiden University news </a> by Erik Arends, <a href="http://phys.org/news/2016-09-frozen-electrons-visualized.html"> Phys.org </a>

- title: Identifying the fingerprint of antiferromagnetic spin fluctuations in iron pnictide superconductors
  image: Pub_FP.png
  description:
    Both the density of states and the QPI dispersion of LiFeAs shows signatures of electron-boson coupling. Comparing these with fingerprints of different boson couplings to conclude that these signatures stem from AF spin fluctuations.
  authors:
    MP Allan* & Kyungmin Lee* & AW Rost*, MH Fischer, F Massee, K Kihou, C-H Lee, A Iyo, H Eisaki, T-M Chuang, AP Mackenzie, JC Davis, DJ Scalapino, E-A Kim
  link:
    url: http://www.nature.com/nphys/journal/v11/n2/full/nphys3187.html
    display: Nature Physics 11, 177 (2015)
  highlight: 1
  news1:
  news2: See also <a href="http://www.news.cornell.edu/stories/2015/01/high-temperature-superconductor-fingerprint-found"> Cornell Chronicle </a> by Anne Ju, <a href="http://phys.org/news/2015-01-high-temperature-superconductor-fingerprint.html"> Phys.org </a>

- title: Direct evidence for a magnetic f-electron mediated Cooper pairing mechanism of heavy fermion superconductivity in CeCoIn5
  image: dummy.png
  description: ""
  authors:
    J Van Dyke, F Massee, MP Allan, JC Davis, C Petrovic, and DK Morr
  link:
    url: http://www.pnas.org/content/111/32/11663
    display: PNAS 111, 11663 (2014)
  highlight: 0
  news1: <a style="color:#FF0000; text-decoration:underline" href="http://science.sciencemag.org/content/345/6203/twil">News item in Science</a>
  news2:

- title: Imaging Cooper pairing of heavy fermions in CeCoIn5
  image: Pub_CeCoIn5.png
  description:
    The heavy Fermion superconductor CeCoIn5 has a d-wave gap with nodes along the (1,1) direction, as we find using QPI.
  authors:
    MP Allan* & F Massee*, DK Morr, J van Dyke, AW Rost, AP Mackenzie, C Petrovic, JC Davis
  link:
    url: http://www.nature.com/nphys/journal/vaop/ncurrent/abs/nphys2671.html
    display: Nature Physics 9, 468 (2013)
  highlight: 1
  news1: <a style="color:#FF0000; text-decoration:underline" href="http://www.nature.com/nphys/journal/v9/n8/full/nphys2708.html">Featured in 'News and Views' by L Taillefer</a>
  news2: See also <a href="http://phys.org/news/2013-07-imaging-electron-pairing-simple-magnetic.html"> Phys.org</a>, <a href="http://davisgroup.lassp.cornell.edu/CeCoIn5_DoE.html">DoE headliner</a>

- title: Formation of heavy d-electron quasiparticles in Sr3Ru2O7
  image: Pub_327.png
  description:
    Using APRES, we find that the bands in Sr3Ru2O7 look more like in heavy fermion material than in a normal metal. We present a detailed investigation into the formation of this particular electronic  structure.
  authors:
    MP Allan, A Tamai, E Rozbicki, MH Fischer, J Voss, PDC King, W Meevasana, S Thirupathaiah, E Rienks, J Fink, A Tennant, RS Perry, JF Mercure, MA Wang, J Lee, CJ Fennie, E-A Kim, MJ Lawler, KM Shen, AP Mackenzie, Z-X Shen, F Baumberger
  link:
    url: http://iopscience.iop.org/1367-2630/15/6/063029
    display: New Journal of Physics 15, 063029 (2013)
  highlight: 1
  news: <a style="color:#FF0000; text-decoration:underline" href="http://iopscience.iop.org/1367-2630/15/6/063029"> Video abstract available</a>

- title: Anisotropic impurity states, quasiparticle scattering and nematic transport in underdoped Ca(Fe1−xCox)2As2
  image: Pub_nemat2b.png
  description:
    We elucidate on the formation of the electronic nematic state in Co-CaFe2As2, finding that the Cobalt dopant atoms play a key role.
  authors:
    MP Allan, T-M Chuang, F Massee, Y Xie, N Ni, SL Bud’ko, GS Boebinger, Q Wang, DS Dessau, PC Canfield, MS Golden, JC Davis
  highlight: 1
  link:
    url: http://www.nature.com/nphys/journal/v9/n4/full/nphys2544.html
    display: Nature Physics 9, 220 (2013)
  news2: See also <a href="http://www.techconnect.org/news/entry.html?id=243"> Tech Connect </a> by Jennifer Rocha, <a href="http://phys.org/news/2013-02-iron-based-superconductor-advances-theory.html"> Phys.org</a> by Bill Steele

- title: Anisotropic energy gaps of iron-based superconductivity from intraband quasiparticle interference in LiFeAs
  image: Pub_LiFeAs.png
  description:
    In contrast to earlier photoemission reports, we show that the gaps in iron based superconductors are not isotropic but modulated with angle.
  authors:
    MP Allan⋆ & AW Rost⋆, AP Mackenzie, Y Xie, JC Davis, K Kihou, CH Lee, A Iyo, H Eisaki, T-M Chuang
  link:
    url: http://www.sciencemag.org/content/336/6081/563
    display: Science 336, 563 (2012)
  highlight: 1
  news2: See also <a href="http://www.news.cornell.edu/stories/2012/05/new-clues-how-iron-superconductors-work"> Cornell Cornincle </a> by Bill Steele, <a href="http://www.eurekalert.org/pub_releases/2012-05/dnl-avo050312.php"> Eureka Alert </a>

- title: How Kondo-holes create intense nanoscale heavy-fermion hybridization disorder
  image: dummy.png
  description: ""
  authors: MH Hamidian, AR Schmidt, IA Firmo, MP Allan, P Bradley, JD Garrett, TJ Williams, GM Luke, Y Dubi, AV Balatsky, JC Davis
  link:
    url: http://www.pnas.org/content/108/45/18233
    display:  PNAS 108, 18233 (2011)
  highlight: 0
  news2:

- title: Nematic electronic structure in the “parent” state of the iron-based superconductor Ca(Fe1−xCox)2As2
  image: Pub_nemat1.png
  description:
    We discovered an electronic nematic in the iron based superconductor Co-CaFe2As2! This state is by now confirmed and quite well characterized  by transport, photoemission, and more SI-STM experiments.
  authors:
    T-M Chuang* & MP Allan*, J Lee, Y Xie, N Ni, SL Bud’ko, GS Boebinger PC Canfield, JC Davis
  link:
    url: http://www.sciencemag.org/content/327/5962/181
    display: Science 327, 181 (2010)
  highlight: 1
  news1: <a style="color:#FF0000" href="http://www.sciencemag.org/content/327/5962/155"> Featured in a ‘Perspective’ by E Fradkin &amp; SA Kivelson</a>
  news2: See also <a  href="https://www.newscientist.com/article/dn18354-electron-spotting-could-explain-warm-superconductors/">Article on NewScientist</a> by Colin Barras

- title: Heavy d-electron quasiparticle interference and real-space electronic structure of Sr3Ru2O7
  image: Pub_hBN.png
  description:
    QPI in Sr327.
  authors: Jinho Lee* & M. P. Allan*, M. A.Wang, J. Farrell, S. A. Grigera, F. Baumberger, J. C. Davis and A. P. Mackenzie
  link:
    url: https://www.nature.com/nphys/journal/v5/n11/full/nphys1397.html
    display: Nature Physics 5, 800 (2009)
  highlight: 0

- title: Fermi surface and van Hove singularities in the itinerant metamagnet Sr3Ru2O7
  image: Pub_hBN.png
  description:
    We determine the Fermi surface of Sr327.
  authors:
    A Tamai, MP Allan, JF Mercure, W Meevasana, R Dunkel, DH Lu, RS Perry, AP Mackenzie, David J Singh, Z-X Shen, F Baumberger
  link:
    url: http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.101.026407
    display: Phys. Rev. Lett. 101, 026407 (2008)
  highlight: 0

- title: Tunable  self-assembly of one-dimensional nanostructures with orthogonal directions
  image: Pub_hBN.png
  description:
    Hexagonal boron nitride shows fancy superstructures on the Mo(111) surface.
  authors:
    MP Allan, S Berner, M Corso, T Greber, J Osterwalder
  link:
    url: https://link.springer.com/article/10.1007/s11671-006-9036-2
    display: Nanoscale Research Letters 2, 94 (2007)
  highlight: 1

- title: Photoelectron Diffraction for a Look inside Nanostructures
  image: dummy.png
  description: ""
  authors:
    J Osterwalder, A Tamai, W Auwärter; MP Allan, T Greber
  link:
    url: hhttp://www.ingentaconnect.com/content/scs/chimia/2006/00000060/00000011/art00013
    display: CHIMIA 11, 759 (2006)
  highlight: 0
